# Sumar[[sumar]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

În acest capitol, ați văzut cum să abordați diferite sarcini NLP utilizând funcția de nivel înalt `pipeline()` din 🤗 Transformers. De asemenea, ați văzut cum să căutați și să utilizați modele în Hub, precum și cum să utilizați API-ul de inferență pentru a testa modelele direct în browser.

Am discutat despre modul în care funcționează modelele Transformer la un nivel general și am vorbit despre importanța învățării prin transfer și a reglării fine. Un aspect cheie este faptul că puteți utiliza întreaga arhitectură sau doar codificatorul sau decodificatorul, în funcție de tipul de sarcină pe care doriți să o rezolvați. Următorul tabel rezumă acest aspect:

| Model           | Exemple                                    | Task-uri                                                                           |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Clasificarea propozițiilor, recunoașterea entităților denumite, Extractive QA    |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Generarea de text                                                                |
| Encoder-decoder | BART, T5, Marian, mBART                    | Rezumare, traducere, răspunsuri generative la întrebări                          |
